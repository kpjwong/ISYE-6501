{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISYE 6501 Homework #8\n",
    "\n",
    "## Jeremy Wong | kwong301@gatech.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11.1\n",
    "\n",
    "Using the crime data set `uscrime.txt` from Questions 8.2, 9.1, and 10.1, build a regression model using:\n",
    "1. Stepwise regression\n",
    "2. Lasso\n",
    "3. Elastic net\n",
    "\n",
    "For Parts 2 and 3, remember to scale the data first – otherwise, the regression coefficients will be on different scales and the constraint won’t have the desired effect. \n",
    "\n",
    "For Parts 2 and 3, use the `glmnet` function in R. Notes on R:\n",
    "- For the elastic net model, what we called λ in the videos, glmnet calls “alpha”; you can get a range of results by varying alpha from 1 (lasso) to 0 (ridge regression) and, of course, other values of alpha in between.\n",
    "- In a function call like glmnet(x,y,family=”mgaussian”,alpha=1) the predictors x need to be in R’s matrix format, rather than data frame format.  You can convert a data frame to a matrix using as.matrix – for example, x <- as.matrix(data[,1:n-1])\n",
    "- Rather than specifying a value of T, glmnet returns models for a variety of values of T. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers to 11.1\n",
    "\n",
    "## Summary\n",
    "- Discussed the methodology, inlucding a review on literature criticisms of stepwise regression in section 1.1.\n",
    "- R implementation discussed in section 1.2, with a introduction of `caret` for future usage.\n",
    "- Reported final model in section 1.3.\n",
    "- Discussed the implemention of glmnet in section 2.1.\n",
    "- Reported final model in section 2.2.\n",
    "\n",
    "## Conclusion\n",
    "- Stepwise model suggested using `M`, `Ed`, `Po1`, `M.F`, `U1`, `U2`, `Ineq`, `Prob` as regressors. \n",
    "- In the final Lasso model, `M`, `So`, `Ed`, `Po1`, `LF`, `M.F`, `NW`, `U1`, `U2`, `Ineq`, `Prob` are selected, while in ridge regression all regressors are selected with shrinked coefficients.\n",
    "- We see that the Lasso model is selected among the class of elastic net models - suggesting that dropping variables in our case here is preferred.\n",
    "- In general, we do not suggest using stepwise regression for formal analysis, because of its issues with data dredging and spurious inference. \n",
    "- Using LOOCV errors alone, we will prefer the Lasso model if we are to choose from CV errors. \n",
    "\n",
    "| Model | Mean CV Error | No. of variables |\n",
    "|:-----:|:-------------:|:----------------:|\n",
    "| Stepwise | 71499      | 8\n",
    "| Lasso | 65813         | 11               |\n",
    "| Ridge | 67889         | 15               |\n",
    "| Elnet | 65813         | 11               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details to Answers\n",
    "\n",
    "# 1.1 Stepwise Regression\n",
    "\n",
    "Coming from an Econometrics background, I'm not very fond of automated stepwise regression approaches that are largely greedy in nature, offer false confidence, and doesn't give insights to model interpretation. [This post](https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856) and [this paper](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-018-0143-6) offer elaborated explanations on why stepwise regression methods can be bad. Essentially, the $p$-values under the t and F tests do not have the claimed distributions in the subsamples for the intermediate/final steps (section 4.3 in [Harrel's notes](https://hbiostat.org/doc/rms.pdf)). Even if selection criterion may not involve p-values/metrics with underlying distribution assumption, the $p$ values and confidence tend to be [over-confident](https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.4780080702), i.e. too small/narrow - because they are chosen according to the performance in tests, creating the illusion that \"significant\" relationships between the variables have been found (in other words, $\\mathbb{E}[\\beta|p_\\beta < 0.05] \\neq \\mathbb{E}[\\beta]$). Also check [this Stata memo](https://www.stata.com/support/faqs/statistics/stepwise-regression-problems/) for review of criticisms to this approach in the literature. \n",
    "\n",
    "These shortcomings typically [arise from interactions between correlated regressors](https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.4780080702). [Heinze, Wallisch and Dunkler (2018)](https://onlinelibrary.wiley.com/doi/full/10.1002/bimj.201700067) provide a nice summary on the mechanism. Check [here](https://www3.nd.edu/~rwilliam/stats1/x91.pdf) for a review on how correlation between regressors affects their standard errors.\n",
    "\n",
    "<img src=\"https://onlinelibrary.wiley.com/cms/asset/a7dc40a8-60b0-48d9-8f35-32c3256d5be0/bimj1842-fig-0002-m.jpg\" width=\"50%\">\n",
    "\n",
    "However, if we are just interested in using a simplified model to make predictions, it would still worth to explore these methods. We introduce three heuristic methods for selecting features in a linear regression model and we will apply the third method, Stepwise Regression which is a generalization of the first two, on the crime data.\n",
    "\n",
    "### Forward variable selection\n",
    "\n",
    "Add factors in the regression until the model performance is satisfactory or we reach the upper bound for the number of variables preset by the user and then trim the model _greedily_ depending on the p-value of each individual variable, as shown in the procedure diagram below.\n",
    "\n",
    "<img src=\"https://github.com/kpjwong/ISYE-6501/blob/main/images/forward_selection.png?raw=true\" width=\"70%\">\n",
    "\n",
    "### Backward variable elimination\n",
    "\n",
    "From a full model with all variables, remove factors until the number of regressors is within a range preset by the user and then stop dropping when we observe a statistically significant variable, as shown below.\n",
    "\n",
    "<img src=\"https://github.com/kpjwong/ISYE-6501/blob/main/images/backward_elimination.png?raw=true\" width=\"70%\">\n",
    "\n",
    "### Stepwise Regression\n",
    "\n",
    "Note that \"stepwise regression\" is a generalization  to variable selection combining the two _greedy_ approaches above. Essentially, it follows the foward selection routine - until we have enough valid variables or the model performance is good enough, we have a combined operation of adding the candidate regressor and eliminate statistically insignificant variables. _Note: I believe there is an error in the \"Yes\" and \"No\" for the \"Is it good enough?\" decision node._\n",
    "\n",
    "<img src=\"https://github.com/kpjwong/ISYE-6501/blob/main/images/stepwise_regression.png?raw=true\" width=\"70%\">\n",
    "\n",
    "### Model Performance Criteria\n",
    "\n",
    "We need some criteria for model performance to make decisions in the decision nodes. Here is a summary with some common criteria:\n",
    "\n",
    "- p-value: This is also the criterion suggested by the slides above. Currently, no R packages support this version. A function written by Paul Rubin supports stepwise regression with this [blog post](https://orinanobworld.blogspot.com/2011/02/stepwise-regression-in-r.html).\n",
    "- AIC: In each step, the AIC's before and after are compared. Variables are added/eliminated based on reduction in AIC, or a cp (complexity parameter) derived from AIC. In R, this is supported by the built-in `step` function and also the `StepAIC` in `MASS` pacakge. These functions has CV wrapper support by `caret` under the alias of `lmStepAIC` and `glmStepAIC`. \n",
    "- Mallow's CP and (gains in) R Squared: both are supported by `regsubsets` function under `leaps` and a `caret` CV wrapper (with alis `leapSeq`). [Mallow's CP](https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/goodness-of-fit-statistics/what-is-mallows-cp/) is a metric similar to AIC which accounts for both goodness-of-fit and model compexity.\n",
    "\n",
    "I am a bit skeptical about using individual p-values for each step greedily. Correlation among variables could give spurious (non-)confidence. E.g. variables can be jointly significant but insignificant individually. AIC is likelihood-based, we've seen in homework 6 that the QQ plots for the residuals look close to normal distribution when we have around 6 or more variables. This justifies the use of AIC. An alternative way would be to use the model-agnostic Mallow's CP for model selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Stepwise Regression Implementation\n",
    "\n",
    "We use the `lmStepAIC` model in `caret` for CV. The base `StepAIC` function:\n",
    "\n",
    "```R\n",
    "stepAIC(object, scope, scale = 0,\n",
    "        direction = c(\"both\", \"backward\", \"forward\"),\n",
    "        trace = 1, keep = NULL, steps = 1000, use.start = FALSE,\n",
    "        k = 2, …)\n",
    "```\n",
    "\n",
    "- `object` needs to be a model object, specifying the base model to be considered in the first step. An example is `lm(y ~ ., data)`, a linear regression with the response regressed on all variables.\n",
    "- `scope` is a list with names upper and lower, which are model objects specifying the scope of search. The upper model will include all candidate variables, while the lower model will include all variables that must be included.\n",
    "- `direction` specifies the mode of stepwise search, can be one of \"both\", \"backward\", or \"forward\", with a default of \"both\". If the scope argument is missing the default for direction is \"backward\".\n",
    "\n",
    "Essentially, the `StepAIC` function internalizes the AIC selection process up to the above input parameters. With the `caret` wrapper, we will just need to specify the CV control/train setup. We'll use a LOOCV and a 10-fold CV repeated 5 times. The `train` function syntax can either be in terms of X and y, or a formula and data.\n",
    "\n",
    "```R\n",
    "## S3 method for class 'default':\n",
    "train(x, y, \n",
    "      method = \"rf\",  \n",
    "      ..., \n",
    "      weights = NULL,\n",
    "      metric = ifelse(is.factor(y), \"Accuracy\", \"RMSE\"),   \n",
    "      maximize = ifelse(metric == \"RMSE\", FALSE, TRUE),\n",
    "      trControl = trainControl(), \n",
    "      tuneGrid = NULL, \n",
    "      tuneLength = 3)\n",
    "\n",
    "## S3 method for class 'formula':\n",
    "train(form, data, ..., weights, subset, na.action, contrasts = NULL)\n",
    "```\n",
    "\n",
    "Here we specify the `method` as `lmStepAIC`, the CV scheme (10 fold, or LOO) is specified in `trControl`. We will use RMSE for the model selection criterion (in CV). Other parameters to be passed to the base method should be included in `...`. For other usage of the package refer to the [offical documentation](https://topepo.github.io/caret/model-training-and-tuning.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Stepwise Regression Model Selection\n",
    "\n",
    "We see that both the 10-fold CV and the LOOCV suggest using `M`, `Ed`, `Po1`, `M.F`, `U1`, `U2`, `Ineq`, `Prob` as regressors. We've seen that `U1` and `U2` are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T21:47:09.386566Z",
     "start_time": "2021-03-17T21:47:07.325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = .outcome ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + \n",
       "    Prob, data = dat)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)            M           Ed          Po1          M.F           U1  \n",
       "   -6426.10        93.32       180.12       102.65        22.34     -6086.63  \n",
       "         U2         Ineq         Prob  \n",
       "     187.35        61.33     -3796.03  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(data.table)\n",
    "library(caret)\n",
    "library(magrittr)\n",
    "\n",
    "crime_dta <- fread('./hw8/uscrime.txt')\n",
    "\n",
    "K_fold_ctrl <- trainControl(method=\"repeatedcv\", number=10, repeats=5)\n",
    "LOO_ctrl <- trainControl(method = \"LOOCV\")\n",
    "\n",
    "set.seed(1)\n",
    "K_fold_stepreg <- train(Crime ~., data = crime_dta ,\n",
    "                        method = \"lmStepAIC\", \n",
    "                        trControl = K_fold_ctrl, trace=FALSE\n",
    "                        )\n",
    "K_fold_stepreg$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T05:09:45.192143Z",
     "start_time": "2021-03-18T05:09:43.204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = .outcome ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + \n",
       "    Prob, data = dat)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)            M           Ed          Po1          M.F           U1  \n",
       "   -6426.10        93.32       180.12       102.65        22.34     -6086.63  \n",
       "         U2         Ineq         Prob  \n",
       "     187.35        61.33     -3796.03  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOO_stepreg <- train(Crime ~., data = crime_dta ,\n",
    "                     method = \"lmStepAIC\", \n",
    "                     trControl = LOO_ctrl, trace=FALSE\n",
    "                        )\n",
    "LOO_stepreg$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T05:42:07.040549Z",
     "start_time": "2021-03-18T05:42:07.019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'method'</li><li>'modelInfo'</li><li>'modelType'</li><li>'results'</li><li>'pred'</li><li>'bestTune'</li><li>'call'</li><li>'dots'</li><li>'metric'</li><li>'control'</li><li>'finalModel'</li><li>'preProcess'</li><li>'trainingData'</li><li>'resample'</li><li>'resampledCM'</li><li>'perfNames'</li><li>'maximize'</li><li>'yLimits'</li><li>'times'</li><li>'levels'</li><li>'terms'</li><li>'coefnames'</li><li>'xlevels'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'method'\n",
       "\\item 'modelInfo'\n",
       "\\item 'modelType'\n",
       "\\item 'results'\n",
       "\\item 'pred'\n",
       "\\item 'bestTune'\n",
       "\\item 'call'\n",
       "\\item 'dots'\n",
       "\\item 'metric'\n",
       "\\item 'control'\n",
       "\\item 'finalModel'\n",
       "\\item 'preProcess'\n",
       "\\item 'trainingData'\n",
       "\\item 'resample'\n",
       "\\item 'resampledCM'\n",
       "\\item 'perfNames'\n",
       "\\item 'maximize'\n",
       "\\item 'yLimits'\n",
       "\\item 'times'\n",
       "\\item 'levels'\n",
       "\\item 'terms'\n",
       "\\item 'coefnames'\n",
       "\\item 'xlevels'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'method'\n",
       "2. 'modelInfo'\n",
       "3. 'modelType'\n",
       "4. 'results'\n",
       "5. 'pred'\n",
       "6. 'bestTune'\n",
       "7. 'call'\n",
       "8. 'dots'\n",
       "9. 'metric'\n",
       "10. 'control'\n",
       "11. 'finalModel'\n",
       "12. 'preProcess'\n",
       "13. 'trainingData'\n",
       "14. 'resample'\n",
       "15. 'resampledCM'\n",
       "16. 'perfNames'\n",
       "17. 'maximize'\n",
       "18. 'yLimits'\n",
       "19. 'times'\n",
       "20. 'levels'\n",
       "21. 'terms'\n",
       "22. 'coefnames'\n",
       "23. 'xlevels'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"method\"       \"modelInfo\"    \"modelType\"    \"results\"      \"pred\"        \n",
       " [6] \"bestTune\"     \"call\"         \"dots\"         \"metric\"       \"control\"     \n",
       "[11] \"finalModel\"   \"preProcess\"   \"trainingData\" \"resample\"     \"resampledCM\" \n",
       "[16] \"perfNames\"    \"maximize\"     \"yLimits\"      \"times\"        \"levels\"      \n",
       "[21] \"terms\"        \"coefnames\"    \"xlevels\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOO_stepreg %>% names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T05:43:46.952539Z",
     "start_time": "2021-03-18T05:43:46.931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "71498.8822140486"
      ],
      "text/latex": [
       "71498.8822140486"
      ],
      "text/markdown": [
       "71498.8822140486"
      ],
      "text/plain": [
       "[1] 71498.88"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CV err\n",
    "(LOO_stepreg$results$RMSE)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "Train on the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T23:33:55.967318Z",
     "start_time": "2021-03-17T23:33:55.938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob, \n",
       "    data = crime_dta)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-444.70 -111.07    3.03  122.15  483.30 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -6426.10    1194.61  -5.379 4.04e-06 ***\n",
       "M              93.32      33.50   2.786  0.00828 ** \n",
       "Ed            180.12      52.75   3.414  0.00153 ** \n",
       "Po1           102.65      15.52   6.613 8.26e-08 ***\n",
       "M.F            22.34      13.60   1.642  0.10874    \n",
       "U1          -6086.63    3339.27  -1.823  0.07622 .  \n",
       "U2            187.35      72.48   2.585  0.01371 *  \n",
       "Ineq           61.33      13.96   4.394 8.63e-05 ***\n",
       "Prob        -3796.03    1490.65  -2.547  0.01505 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 195.5 on 38 degrees of freedom\n",
       "Multiple R-squared:  0.7888,\tAdjusted R-squared:  0.7444 \n",
       "F-statistic: 17.74 on 8 and 38 DF,  p-value: 1.159e-10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " lm(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob, crime_dta) %>% summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T23:34:11.692449Z",
     "start_time": "2021-03-17T23:34:11.667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Crime ~ M + Ed + Po1 + M.F + U2 + Ineq + Prob, data = crime_dta)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-433.83 -121.23    2.28  125.26  551.58 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -5666.91    1152.50  -4.917 1.63e-05 ***\n",
       "M              97.87      34.38   2.846  0.00701 ** \n",
       "Ed            170.22      54.01   3.151  0.00312 ** \n",
       "Po1           116.58      13.91   8.382 2.95e-10 ***\n",
       "M.F            10.84      12.40   0.874  0.38750    \n",
       "U2             79.01      42.71   1.850  0.07190 .  \n",
       "Ineq           65.69      14.16   4.640 3.87e-05 ***\n",
       "Prob        -3858.20    1533.99  -2.515  0.01613 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 201.3 on 39 degrees of freedom\n",
       "Multiple R-squared:  0.7704,\tAdjusted R-squared:  0.7291 \n",
       "F-statistic: 18.69 on 7 and 39 DF,  p-value: 1.173e-10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " lm(Crime ~ M + Ed + Po1 + M.F + U2 + Ineq + Prob, crime_dta) %>% summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Lasso/Elastic Net Implementation\n",
    "\n",
    "Lasso and elastic net implementation belongs to the class of regularized linear regression, which minimizes the following regularized loss function:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\beta; \\alpha, \\lambda) &= \\sum_{i=1}^n (y_i - \\beta x_i)^2 + \\lambda \\bigg[ \\alpha \\Vert \\beta \\Vert_1 + \\frac{1-\\alpha}{2} \\Vert \\beta \\Vert^2_2 \\bigg]\n",
    "\\end{align*}\n",
    "\n",
    "- $\\lambda \\geq 0$ is the complexity parameter, $\\alpha \\in [0,1]$ determines the model specification.\n",
    "- If $\\alpha = 0$, it is a ridge regression.\n",
    "- If $\\alpha = 1$, it is a lasso regression.\n",
    "- If $\\alpha \\in (0,1)$, it is an elastic net.\n",
    "\n",
    "We'll use the R package `glmnet` to train the model. The R syntax is:\n",
    "\n",
    "```R\n",
    "glmnet(\n",
    "    x,\n",
    "    y,\n",
    "    family = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\", \"cox\", \"mgaussian\"),\n",
    "    alpha = 1,\n",
    "    nlambda = 100,\n",
    "    lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04),\n",
    "    standardize = TRUE,\n",
    "    thresh = 1e-07,\n",
    "    dfmax = nvars + 1,\n",
    "    pmax = min(dfmax * 2 + 20, nvars),\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "Notable input parameters:\n",
    "\n",
    "- `x`, `y` are the independent and response variables. Note that they need to be in matrix forms.\n",
    "- `family` specifies the distribution of the error term, which \n",
    "- `alpha` is the same as the parameter $\\alpha$ in the equation above. Zero imples ridge regression and one implies lasso.\n",
    "- `nlambda` is the number of potential complexity parameter $\\lambda$ to be evaluated. Note that `glmnet` internalize the selection of $\\lambda$. \n",
    "- `lambda.min.ratio` is the ratio of minimum lambda relative to the maximum lambda to be considered. Note that the maximum lambda is the smallest value for which all the coefficients in $\\beta$ are zero.\n",
    "- `standardize` is a logical flag for x variable standardization, prior to fitting the model sequence. The coefficients are always returned on the original scale, so we don't have to do this beforehand.\n",
    "- `thresh` is the convergence criterion for the algorithm.\n",
    "- `dfmax` is the maximum limit on the number of variables in the model. Useful for very large nvars, if a partial path is desired.\n",
    "- `pmax` is the the maximum number of variables ever allowed to be nonzero.\n",
    "\n",
    "Notable outputs and methods:\n",
    "\n",
    "- `.$dev.ratio`: The fraction of deviance explained / R squared in the case of elastic net / Lasso or ridge.\n",
    "- `.$nulldev`: The per-observation deviance under null (intercept only model), this can be used to derive the model RMSE.\n",
    "- `print()`: Print the deviance ratio for each step.\n",
    "- `predict()`: Make predictions at given values of X.\n",
    "- `plot()`: Plot the coefficients across lambdas.\n",
    "- `cv.glmnet()`: A built-in CV routine to select glmnet model with the best lambda. Refer [here](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) for a demo.\n",
    "\n",
    "`caret` does provide wrapper support for glmnet. It allows us to use CV to tune lambda and alpha simultaneously through a grid search (grid points specified by the `tuneGrid` parameter). It is however noted that the [lambda output is probably not optimized](https://stats.stackexchange.com/questions/327239/r-coefficients-from-glmnet-and-caret-are-different-for-the-same-lambda). So we'll iterate alpha over a loop of `cv.glmnet` calls. We'll use LOOCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Results\n",
    "\n",
    "Here's the pure lasso, ridge and elastic net model selection results using LOOCV. Note that the Elastic Net had the lowest mean CV error at alpha=1, which is the lasso case.\n",
    "\n",
    "| Model | Mean CV Error | No. of variables |\n",
    "|:-----:|:-------------:|:----------------:|\n",
    "| Lasso | 65813         | 11               |\n",
    "| Ridge | 67889         | 15               |\n",
    "| Elnet | 65813         | 11               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T05:31:27.253643Z",
     "start_time": "2021-03-18T05:31:27.029Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  cv.glmnet(x = crime_dta[, -\"Crime\"] %>% as.matrix, y = crime_dta[,      \"Crime\"] %>% as.matrix, nfolds = nrow(crime_dta), alpha = 1,      family = \"gaussian\") \n",
       "\n",
       "Measure: Mean-Squared Error \n",
       "\n",
       "    Lambda Index Measure    SE Nonzero\n",
       "min  13.40    33   65813 13349      11\n",
       "1se  37.29    22   76484 15389       5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lasso_mod <- cv.glmnet(x=crime_dta[, -\"Crime\"] %>% as.matrix,\n",
    "                       y=crime_dta[, \"Crime\"] %>% as.matrix,\n",
    "                       nfolds=nrow(crime_dta),\n",
    "                       alpha=1, family=\"gaussian\")\n",
    "\n",
    "lasso_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T05:31:41.143381Z",
     "start_time": "2021-03-18T05:31:40.941Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  cv.glmnet(x = crime_dta[, -\"Crime\"] %>% as.matrix, y = crime_dta[,      \"Crime\"] %>% as.matrix, nfolds = nrow(crime_dta), alpha = 0,      family = \"gaussian\") \n",
       "\n",
       "Measure: Mean-Squared Error \n",
       "\n",
       "    Lambda Index Measure    SE Nonzero\n",
       "min  45.98    94   67889 12486      15\n",
       "1se 295.54    74   79983 15210      15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge_mod <- cv.glmnet(x=crime_dta[, -\"Crime\"] %>% as.matrix,\n",
    "                       y=crime_dta[, \"Crime\"] %>% as.matrix,\n",
    "                       nfolds=nrow(crime_dta),\n",
    "                       alpha=0, family=\"gaussian\")\n",
    "\n",
    "ridge_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T05:32:34.837418Z",
     "start_time": "2021-03-18T05:32:30.545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 21 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>alpha</th><th scope=col>lambda</th><th scope=col>CV_err</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>45.976668</td><td>67888.93</td></tr>\n",
       "\t<tr><td>0.05</td><td>41.699721</td><td>67551.41</td></tr>\n",
       "\t<tr><td>0.10</td><td>36.435723</td><td>67458.89</td></tr>\n",
       "\t<tr><td>0.15</td><td>29.257972</td><td>67438.34</td></tr>\n",
       "\t<tr><td>0.20</td><td>26.430998</td><td>67574.58</td></tr>\n",
       "\t<tr><td>0.25</td><td>23.206390</td><td>67771.06</td></tr>\n",
       "\t<tr><td>0.30</td><td>19.338658</td><td>67940.14</td></tr>\n",
       "\t<tr><td>0.35</td><td>18.192132</td><td>68079.01</td></tr>\n",
       "\t<tr><td>0.40</td><td>15.918115</td><td>68198.12</td></tr>\n",
       "\t<tr><td>0.45</td><td>15.528988</td><td>68261.77</td></tr>\n",
       "\t<tr><td>0.50</td><td>13.976089</td><td>68321.77</td></tr>\n",
       "\t<tr><td>0.55</td><td>11.576811</td><td>68290.85</td></tr>\n",
       "\t<tr><td>0.60</td><td> 8.027647</td><td>67988.94</td></tr>\n",
       "\t<tr><td>0.65</td><td> 9.795763</td><td>67596.47</td></tr>\n",
       "\t<tr><td>0.70</td><td> 9.096066</td><td>67220.57</td></tr>\n",
       "\t<tr><td>0.75</td><td> 9.317393</td><td>66880.03</td></tr>\n",
       "\t<tr><td>0.80</td><td>10.521406</td><td>66598.02</td></tr>\n",
       "\t<tr><td>0.85</td><td>10.867981</td><td>66326.27</td></tr>\n",
       "\t<tr><td>0.90</td><td>10.264204</td><td>66141.74</td></tr>\n",
       "\t<tr><td>0.95</td><td>10.672059</td><td>65939.51</td></tr>\n",
       "\t<tr><td>1.00</td><td>13.402443</td><td>65813.23</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 21 × 3\n",
       "\\begin{tabular}{lll}\n",
       " alpha & lambda & CV\\_err\\\\\n",
       " <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 45.976668 & 67888.93\\\\\n",
       "\t 0.05 & 41.699721 & 67551.41\\\\\n",
       "\t 0.10 & 36.435723 & 67458.89\\\\\n",
       "\t 0.15 & 29.257972 & 67438.34\\\\\n",
       "\t 0.20 & 26.430998 & 67574.58\\\\\n",
       "\t 0.25 & 23.206390 & 67771.06\\\\\n",
       "\t 0.30 & 19.338658 & 67940.14\\\\\n",
       "\t 0.35 & 18.192132 & 68079.01\\\\\n",
       "\t 0.40 & 15.918115 & 68198.12\\\\\n",
       "\t 0.45 & 15.528988 & 68261.77\\\\\n",
       "\t 0.50 & 13.976089 & 68321.77\\\\\n",
       "\t 0.55 & 11.576811 & 68290.85\\\\\n",
       "\t 0.60 &  8.027647 & 67988.94\\\\\n",
       "\t 0.65 &  9.795763 & 67596.47\\\\\n",
       "\t 0.70 &  9.096066 & 67220.57\\\\\n",
       "\t 0.75 &  9.317393 & 66880.03\\\\\n",
       "\t 0.80 & 10.521406 & 66598.02\\\\\n",
       "\t 0.85 & 10.867981 & 66326.27\\\\\n",
       "\t 0.90 & 10.264204 & 66141.74\\\\\n",
       "\t 0.95 & 10.672059 & 65939.51\\\\\n",
       "\t 1.00 & 13.402443 & 65813.23\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 21 × 3\n",
       "\n",
       "| alpha &lt;dbl&gt; | lambda &lt;dbl&gt; | CV_err &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 0.00 | 45.976668 | 67888.93 |\n",
       "| 0.05 | 41.699721 | 67551.41 |\n",
       "| 0.10 | 36.435723 | 67458.89 |\n",
       "| 0.15 | 29.257972 | 67438.34 |\n",
       "| 0.20 | 26.430998 | 67574.58 |\n",
       "| 0.25 | 23.206390 | 67771.06 |\n",
       "| 0.30 | 19.338658 | 67940.14 |\n",
       "| 0.35 | 18.192132 | 68079.01 |\n",
       "| 0.40 | 15.918115 | 68198.12 |\n",
       "| 0.45 | 15.528988 | 68261.77 |\n",
       "| 0.50 | 13.976089 | 68321.77 |\n",
       "| 0.55 | 11.576811 | 68290.85 |\n",
       "| 0.60 |  8.027647 | 67988.94 |\n",
       "| 0.65 |  9.795763 | 67596.47 |\n",
       "| 0.70 |  9.096066 | 67220.57 |\n",
       "| 0.75 |  9.317393 | 66880.03 |\n",
       "| 0.80 | 10.521406 | 66598.02 |\n",
       "| 0.85 | 10.867981 | 66326.27 |\n",
       "| 0.90 | 10.264204 | 66141.74 |\n",
       "| 0.95 | 10.672059 | 65939.51 |\n",
       "| 1.00 | 13.402443 | 65813.23 |\n",
       "\n"
      ],
      "text/plain": [
       "   alpha lambda    CV_err  \n",
       "1  0.00  45.976668 67888.93\n",
       "2  0.05  41.699721 67551.41\n",
       "3  0.10  36.435723 67458.89\n",
       "4  0.15  29.257972 67438.34\n",
       "5  0.20  26.430998 67574.58\n",
       "6  0.25  23.206390 67771.06\n",
       "7  0.30  19.338658 67940.14\n",
       "8  0.35  18.192132 68079.01\n",
       "9  0.40  15.918115 68198.12\n",
       "10 0.45  15.528988 68261.77\n",
       "11 0.50  13.976089 68321.77\n",
       "12 0.55  11.576811 68290.85\n",
       "13 0.60   8.027647 67988.94\n",
       "14 0.65   9.795763 67596.47\n",
       "15 0.70   9.096066 67220.57\n",
       "16 0.75   9.317393 66880.03\n",
       "17 0.80  10.521406 66598.02\n",
       "18 0.85  10.867981 66326.27\n",
       "19 0.90  10.264204 66141.74\n",
       "20 0.95  10.672059 65939.51\n",
       "21 1.00  13.402443 65813.23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(warn=-1)\n",
    "\n",
    "CV_err_list <- list()\n",
    "counter <- 1\n",
    "\n",
    "alpha_grid <- seq(0, 1, length=21)\n",
    "for (alpha in alpha_grid) {\n",
    "    mod <- cv.glmnet(x=crime_dta[, -\"Crime\"] %>% as.matrix,\n",
    "                     y=crime_dta[, \"Crime\"] %>% as.matrix,\n",
    "                     nfolds=nrow(crime_dta),\n",
    "                     alpha=alpha, family=\"gaussian\")\n",
    "    CV_err_list[[counter]] <- data.table(alpha=alpha, lambda=mod$lambda.min, CV_err=mod$cvm[mod$index[1]])\n",
    "    counter <- counter + 1\n",
    "}\n",
    "CV_err <- CV_err_list %>% rbindlist\n",
    "CV_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net coincides with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T05:41:40.385868Z",
     "start_time": "2021-03-18T05:41:40.361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 1 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>alpha</th><th scope=col>lambda</th><th scope=col>CV_err</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>13.40244</td><td>65813.23</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 1 × 3\n",
       "\\begin{tabular}{lll}\n",
       " alpha & lambda & CV\\_err\\\\\n",
       " <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 13.40244 & 65813.23\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 1 × 3\n",
       "\n",
       "| alpha &lt;dbl&gt; | lambda &lt;dbl&gt; | CV_err &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | 13.40244 | 65813.23 |\n",
       "\n"
      ],
      "text/plain": [
       "  alpha lambda   CV_err  \n",
       "1 1     13.40244 65813.23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CV_err[which.min(CV_err)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Selection in Final Lasso Model\n",
    "\n",
    "`M`, `So`, `Ed`, `Po1`, `LF`, `M.F`, `NW`, `U1`, `U2`, `Ineq`, `Prob` are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T05:47:38.319971Z",
     "start_time": "2021-03-18T05:47:38.294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15 x 1 sparse Matrix of class \"dgCMatrix\"\n",
       "                  s0\n",
       "M         64.1560942\n",
       "So        48.7815935\n",
       "Ed        95.5282426\n",
       "Po1      105.0198017\n",
       "Po2        .        \n",
       "LF        86.7265767\n",
       "M.F       16.0218862\n",
       "Pop        .        \n",
       "NW         0.2318588\n",
       "U1      -232.4988470\n",
       "U2        42.8035013\n",
       "Wealth     .        \n",
       "Ineq      42.7106536\n",
       "Prob   -3463.4094801\n",
       "Time       .        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lasso_mod <- glmnet(x=crime_dta[, -\"Crime\"] %>% as.matrix,\n",
    "                    y=crime_dta[, \"Crime\"] %>% as.matrix,\n",
    "                    alpha=1, lambda=CV_err[which.min(CV_err), lambda], family=\"gaussian\")\n",
    "\n",
    "lasso_mod$beta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
