{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction and Classification\n",
    "\n",
    "## Some glossary\n",
    "\n",
    "- Hard classifier is one that neccessitates no misclassification in its training/estimation. Soft classifier minimizes the weighted errors for misclassification, and the fitted model might have missclassified examples in the training data.\n",
    "- The loss function in classifiers can be weighted according to ad hoc importance associated to each class. E.g. the classification of toxic mushrooms vs edible mushrooms, loan qualities etc. \n",
    "- A higher relative weight pushes the boundary away from that particular class. Bear in mind that for soft classifiers as the relative weight get too large the boundary will be so far away that potentially includes data of other classes.\n",
    "- Structured data refers to datasets that can be stored in structured ways, e.g. numerical, string arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (Linear)\n",
    "\n",
    "Check [here](http://pyml.sourceforge.net/doc/howto.pdf) for a complete cover. A support vector is a vector that bounds the convex hull formed by the data points in a specific class. Notably, support vectors necessarily include data samples on the margin of the convex hull. The goal here is to find a set of separating hyperlines defined by a vector of coefficients (hence are parallel) that classifies the classes/minimizes (weighted) misclassification errors, and that the distance between the hyperplane and the nearest point from either group is maximized.\n",
    "\n",
    "## Hard margin linear formulation\n",
    "\n",
    "The mathematical formulation for a __linear, hard margin__ SVM problem with binary classes $y_i \\in \\{-1,1\\}$ and data points $(x_1, x_2, \\ldots, x_n)$ where $x_i$ does not include the constant, is:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\max_{a,b} D(a, b; x_1, x_2, \\ldots, x_n)\\\\\n",
    "&s.t. \\begin{cases}\n",
    "a^T x_i + b \\geq 1, & y_i = 1 \\\\\n",
    "a^T x_i + b \\leq -1, & y_i = -1 \\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "We note that the distance metric $D(a, b; x_1, x_2, \\ldots, x_n)$ is given by the distance between hyperplanes that are defined by $a^T x + b = 1$ and $a^T x + b = -1$. To derive this, note that the unit normal vector that is paralleled to the either plane is $\\frac{a}{\\Vert a \\Vert}$. Suppose $z$ is on $a^T x + b = -1$, then $z + t(\\frac{a}{\\Vert a \\Vert})$ where t is the distance between the planes will be on $a^T x + b = 1$.\n",
    "\n",
    "\\begin{align*}\n",
    "a^T\\big(z + t\\frac{a}{\\Vert a \\Vert}\\big) + b &= 1 \\\\\n",
    "(a^T z + b) + t \\frac{\\Vert a \\Vert^2}{\\Vert a \\Vert}  &= 1\\\\\n",
    "(-1) + t \\Vert a \\Vert &= 1\\\\\n",
    "t &= \\frac{2}{\\Vert a \\Vert}\n",
    "\\end{align*}\n",
    "\n",
    "Also noting that the constraints can be expressed as $(a^T x_i + b)y_i \\geq 1$ we express the SVM problem as:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\min_{a,b} \\frac{1}{2}{\\Vert a \\Vert^2}\\\\\n",
    "&s.t. (a^T x_i + b)y_i \\geq 1 \\text{   } \\forall i\n",
    "\\end{align*}\n",
    "\n",
    "When we get SVM $(a,b)$ we can adjust \"zero\" to account for ad hoc penalty of misclassification. For example, if a hard margin SVM is defined by $a^T x + b = 0$. We can penalize misclassification into this class by adding a constant (absolute value < 1). Suppose we want to penalize $y=1$ , the event of loan repayment so that the bank does not overlend, we can add a constant, say $\\frac{1}{3}$ such the $y=1$ only if $a^T x + b \\geq \\frac{1}{3}$.\n",
    "\n",
    "## Soft margin linear formulation\n",
    "\n",
    "Check [here](https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe) for more discussion. Recall that a SMV has a boundary defined by some discriminary function $f$, e.g. linear function $a^T x +b$ as we have been studying so far. The decision __boundary__ is defined by $f(x) = 0$, while the __margins__ are defined by $f(x) = \\pm 1$. A hard margin SVM requires that all points with $y = \\pm1$ lie within $f(x) = \\pm 1$, i.e. strict enforcement of $f(x_i) y_i \\geq 1$. A soft margin classifier, on the other hand, tolerates out-of-margin or even misclassified data points in the training process. We would prefer soft to hard margin classifier as the data itself might not be linearly separable, or to avoid overfitting when outliers are being forced to lie on the correct side, distorting the boundary. Instead of having margin inclusion as hard constraints, we now minimize both the classification error and the norm of weights. Recall that $(a^T x + b)y \\geq 1$ implies no error so the classification error can be expressed as $\\max\\{0, 1-(a^T x +b)y\\}$. Therefore, we express the soft margin linear SVM training problem as:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\min_{a,b} \\sum_i \\max\\{0, 1-(a^T x_i +b)y_i\\} + \\frac{\\lambda}{2}\\Vert a \\Vert^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Here $\\lambda$ is the tradeoff between increasing the margin size and ensuring that the data points lie on the correct side of the margin. If we want to specify that importance/contribute vary across data points we can attribute weight $w_i$ to the $i$-th observation, as in:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{a,b} \\sum_i w_i \\max\\{0, 1-(a^T x_i +b)y_i\\} + \\frac{1}{2}\\Vert a \\Vert^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "$\\lambda$ dissipated because when relative importance of $w_i$ vary it suffices to normalize importance of minimizing $\\Vert a \\Vert^2$ to 1. Some examples of weighing $w_i$ can be: assigned by $y_i$ if a particular outcome is rare or of different importance than the other. Data known to be boundary case and misclassified often, etc. Note that there is a Lagrange multiplier interpretation of $w_i$, by rewriting the problem as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{a,b} \\sum_i w_i \\big((a^T x_i +b)y_i-1\\big) + \\frac{1}{2}\\Vert a \\Vert^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Here $w_i = 0$ if $(a^T x_i +b)y_i - 1< 0$ and $w_i > 0$ otherwise. In other words, this complementary slackness implies a weight assignment determined by whether the data point lies on the correct region. In the literature, it is perhaps more common to express this in terms of slack variables $\\xi_i$ (check [here](https://math.stackexchange.com/questions/3133125/why-do-derivations-for-svm-not-consider-slack-variables-for-inequality-constrain) for a discussion on the equivalence):\n",
    "\n",
    "\\begin{align*}\n",
    "&\\min_{a,b} C\\sum_i \\xi_i + \\frac{1}{2}{\\Vert a \\Vert^2}\\\\\n",
    "&s.t. (a^T x_i + b)y_i \\geq 1 - \\xi_i, \\xi_i \\geq 0 \\text{   } \\forall i\n",
    "\\end{align*}\n",
    "\n",
    "The intuition on values of $\\xi_i$: 0 means within the margin, between 0-1 implies inside the margin, >1 means out of the margin. A smaller value of $C$ allows the classifier to ignore points close to the boundary, and increases the margin. \n",
    "\n",
    "## Need for feature engineering\n",
    "\n",
    "Models that are trained on distance metrics defined as functions of data values will require rescaling so that the distance will be unit-free. E.g. we would not want to have variations in large unit variables (e.g. SAT score) to dominate variables with smaller units (e.g. age). This will also apply to SVM. We can either compress the variables to a uniform (scaling) or normal (standardizing):\n",
    "1. Scaling usually applies to data known with bounds, e.g. RGS color intensities (0-255), SAT scores (200-800). It is also applied to neural network variables.\n",
    "2. Standardization applies to PCA and clustering.\n",
    "\n",
    "Usually, standardization is preferred, in general. Zero-mean and unit-variance make it easier to learn the weights. It also maintains useful information in outliers and makes the algorithm less sensitive to them (relative to min-max scaling). If unsure, it is always a good idea to try both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor Classifier\n",
    "\n",
    "Notes from previous read of Python Machine Learning (Raschka):\n",
    "- Non-parametric classifier:\n",
    "    1. Choose a natural number k and a distance metric (Manhattan, Eucliean, Minkowski etc)\n",
    "    2. For each point to be classified, find the k nearest samples wrt chosen metric\n",
    "    3. Prediction of class labels by majority vote\n",
    "    4. As KNN are trained on distance concepts, features must be standardized to prevent bias against large scales.\n",
    "- Hyper-parameter: $k$ – overfitting-performance tradeoff, distance – needs to be meaningful.\n",
    "- Advantage: Simple to implement, handles multi-class cases, do well with enough representative data. Disadvantage: computationally expensive, the need to store data.\n",
    "- Disadvantage: it cannot be regularized, prone to curse of dimensionality and overfitting: if we would keep adding features, the dimensionality of the feature space grows, and becomes sparser and sparser. Due to this sparsity, it becomes much more easy to find a separable hyperplane because the likelihood that a training sample lies on the wrong side of the best hyperplane becomes infinitely small when the number of features becomes infinitely large (but sample size remains finite). Check [here](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/) for a detailed explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (Kernel)\n",
    "\n",
    "Linearly separability is rare in real life data set. In order to account for this in SVM, a straightforward workaround would be to map the feature space $X$ (or add non-linear terms) to a (potentially different dimension) transformed feature space $\\phi(X)$ that exhibits linear separability. The resulting separating function becomes $a^T \\phi(x) + b = 1$, which permits non-linearity in shape.\n",
    "\n",
    "Such transformation, despite intuitive, will be computationally expensive. For example, if $\\phi$ is the square operator, the number of terms increases linearly ($C^k_2$ cross terms). Fortunately, the kernel trick allows us to scale these transformed SVM efficiently. The kernel trick utilitzes two assumptions:\n",
    "\n",
    "1. The kernel function associated with mapping $\\phi$, $\\kappa(x,x') = \\phi^T(x)\\phi(x)$ can be evaluated easily. \n",
    "2. Optimal weight vector $a$ can be expressed as a linear combination of the training examples $\\sum_i \\alpha_i x_i$. In the transformed feature space, $a = \\sum_i \\alpha_i \\phi(x_i)$.\n",
    "\n",
    "The motivation is that the kernel function will be evaluated numerous times. \n",
    "\n",
    "\\begin{align*}\n",
    "&a^T \\phi(x) + b \\\\\n",
    "=&\\big(\\sum_i \\alpha_i \\phi(x_i)\\big)^T \\phi(x) +b\\\\\n",
    "=&\\sum_i \\alpha_i \\phi(x_i)^T\\phi(x) + b\\\\\n",
    "=&\\sum_i \\alpha_i \\kappa(x_i, x) + b\n",
    "\\end{align*}\n",
    "\n",
    "An example is the quadratic transformation on a 2-dimensional feature space. Suppose $\\phi(x) = [x_1^2, \\sqrt{2}x_1 x_2, x_2^2]$. The kernel function is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi^T(x)\\phi(z) &= (x_1^2, \\sqrt{2}x_1 x_2, x_2^2)^T(z_1^2, \\sqrt{2}z_1 z_2, z_2^2)\\\\\n",
    "&= (x_1 z_1)^2 + 2 x_1 x_2 z_1 z_2 + (x_2 z_2)^2 \\\\\n",
    "&= (x^Tz)^2\n",
    "\\end{align*}\n",
    "\n",
    "Popular kernels include polynomial kernel, sigmoid, Gaussian kernel, etc ([more](https://data-flair.training/blogs/svm-kernel-functions/)). Note that even though the kernel trick is motivated by a mapping $\\phi$ we only need the kernel functions. For example, the Gaussian kernel given by $\\kappa(x,x') = \\exp(-\\gamma\\Vert x-x'\\Vert^2)$ does not have a mapping with explicit form $\\phi$ (check [here](https://stats.stackexchange.com/questions/69759/feature-map-for-the-gaussian-kernel) for more detailed discussion - turns out, the mapped feature space is of infinite dimension).\n",
    "\n",
    "# SVM with Unbalanced Data\n",
    "\n",
    "Caution needs to be taken when applying classifiers to datasets with unbalanced classes (very rare occassions of a particular class label). First, there might not be sufficient examples to produce consistent fits. Even if there are sufficient examples in the entire dataset, information loss will be incurred in the train-test split if the process does not ensure stratified subsets of data. Finally, because one event (WLOG, assume the positive response) is rare, it will be hard to properly gauge performance - even a classifier that predicts purely the dominant labels will result in a low error rate. Mathematically,\n",
    "\n",
    "\\begin{align*}\n",
    "P(success) = P(success|+)P(+) + P(success|-)P(-)\n",
    "\\end{align*}\n",
    "\n",
    "A modified metric would be the balanced success rate defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{P}(success) = \\frac{P(success|+)+P(success|-)}{2} \\triangleq BSR\n",
    "\\end{align*}\n",
    "\n",
    "The balanced error rate is defined as $1-BSR$. The concept on weighing success (thereby error) rates across classes is implemented in SVM through the hyper-parameter $C$. Instead of having a uniform contribution of error to the objective across all data points, we assign weights according to their classes: $C\\sum_i \\xi_i \\leftarrow C_+ \\sum_{I_+}\\xi_i + C_- \\sum_{I_-}\\xi_i$. Note that the total contribution will be $C_+n_+$ and $C_-n_-$ for both classes, equal contribution implies $\\frac{C_+}{C_-} = \\frac{n_-}{n_+}$. So we can have $C_\\pm = \\lambda n_\\mp$ where $\\lambda$ controls the overall magnitude for regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2\n",
    "\n",
    "The files `credit_card_data.txt` (without headers) and `credit_card_data-headers.txt` (with headers) contain a dataset with 654 data points, 6 continuous and 4 binary predictor variables.  It has anonymized credit card applications with a binary response variable (last column) indicating if the application was positive or negative. The dataset is the “Credit Approval Data Set” from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Credit+Approval) without the categorical variables and without data points that have missing values.\n",
    "\n",
    "1.\tUsing the support vector machine function ksvm contained in the R package kernlab, find a good classifier for this data. Show the equation of your classifier, and how well it classifies the data points in the full data set.  (Don’t worry about test/validation data yet; we’ll cover that topic soon.)\n",
    "\n",
    "2.\tYou are welcome, but not required, to try other (nonlinear) kernels as well; we’re not covering them in this course, but they can sometimes be useful and might provide better predictions than vanilladot.\n",
    "\n",
    "3.\tUsing the k-nearest-neighbors classification function kknn contained in the R kknn package, suggest a good value of k, and show how well it classifies that data points in the full data set.  Don’t forget to scale the data (scale=TRUE in kknn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Some Tips__\n",
    "\n",
    "1. On `ksvm`\n",
    "\n",
    "    - You can use scaled=TRUE to get ksvm to scale the data as part of calculating a classifier.\n",
    "    - The term λ we used in the SVM lesson to trade off the two components of correctness and margin is called C in ksvm. One of the challenges of this homework is to find a value of C that works well; for many values of C, almost all predictions will be “yes” or almost all predictions will be “no”.\n",
    "    - ksvm does not directly return the coefficients a0 and a1…am. Instead, you need to do the last step of the calculation yourself. Here’s an example of the steps to take (assuming your data is stored in a matrix called data): \n",
    "\n",
    "    ```R\n",
    "    # call ksvm.  Vanilladot is a simple linear kernel.\n",
    "    model <- ksvm(data[,1:10],data[,11],type=”C-svc”,kernel=”vanilladot”,C=100,scaled=TRUE)\n",
    "    # calculate a1…am\n",
    "    a <- colSums(model@xmatrix[[1]] * model@coef[[1]])\n",
    "    a\n",
    "    # calculate a0\n",
    "    a0 <- –model@b\n",
    "    a0\n",
    "    # see what the model predicts\n",
    "    pred <- predict(model,data[,1:10])\n",
    "    pred\n",
    "    # see what fraction of the model’s predictions match the actual classification\n",
    "    sum(pred == data[,11]) / nrow(data)\n",
    "    ```\n",
    "2. On `kknn`\n",
    "\n",
    "    - You need to be a little careful. If you give it the whole data set to find the closest points to i, it’ll use i itself (which is in the data set) as one of the nearest neighbors. A helpful feature of R is the index –i, which means “all indices except i”.  For example, data[-i,] is all the data except for the ith data point. For our data file where the first 10 columns are predictors and the 11th column is the response, data[-i,11] is the response for all but the ith data point, and data[-i,1:10] are the predictors for all but the ith data point. (There are other, easier ways to get around this problem, but I want you to get practice doing some basic data manipulation and extraction, and maybe some looping too.)\n",
    "    - Note that kknn will read the responses as continuous, and return the fraction of the k closest responses that are 1 (rather than the most common response, 1 or 0).\n",
    "\n",
    "3. Other tips\n",
    "\n",
    "Hint: You might want to view the predictions your model makes; if C is too large or too small, they’ll almost all be the same (all zero or all one) and the predictive value of the model will be poor.  Even finding the right order of magnitude for C might take a little trial-and-error.\n",
    "\n",
    "Note: If you get the error “Error in vanilladot(length = 4, lambda = 0.5) : unused arguments (length = 4, lambda = 0.5)”, it means you need to convert data into matrix format:\n",
    "\n",
    "```R\n",
    "model <- ksvm(as.matrix(data[,1:10]),as.factor(data[,11]),type=”C-svc”,kernel=”vanilladot”,C=100,scaled=TRUE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer for 2.2 (1)\n",
    "\n",
    "#### Data Inspection\n",
    "\n",
    "We apply the linear SVM (soft margin) on the credit card data, which has 10 numerical features encoded as `A*` and a class `R1` that we target to fit and predict. We see below that the ranges and scales for the features vary, so we need to perform some kind of scaling to the data (which will be internalized by `kernlab`). The class label `R1` is rather balanced with a roughly 9:11 ratio - this should render the dataset balanced and a single regularization parameter should suffice. \n",
    "\n",
    "We would probably want to __standardize__ the data instead of a min-max scaling, at least for variable `A15`. It has rather high variance and likely some outliers. Standardization allows the model to maintain useful information in outliers and makes the algorithm less sensitive to them relative to min-max scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T07:02:54.953532Z",
     "start_time": "2021-01-18T07:02:54.912Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       A1               A2              A3               A8        \n",
       " Min.   :0.0000   Min.   :13.75   Min.   : 0.000   Min.   : 0.000  \n",
       " 1st Qu.:0.0000   1st Qu.:22.58   1st Qu.: 1.040   1st Qu.: 0.165  \n",
       " Median :1.0000   Median :28.46   Median : 2.855   Median : 1.000  \n",
       " Mean   :0.6896   Mean   :31.58   Mean   : 4.831   Mean   : 2.242  \n",
       " 3rd Qu.:1.0000   3rd Qu.:38.25   3rd Qu.: 7.438   3rd Qu.: 2.615  \n",
       " Max.   :1.0000   Max.   :80.25   Max.   :28.000   Max.   :28.500  \n",
       "       A9              A10              A11              A12        \n",
       " Min.   :0.0000   Min.   :0.0000   Min.   : 0.000   Min.   :0.0000  \n",
       " 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.:0.0000  \n",
       " Median :1.0000   Median :1.0000   Median : 0.000   Median :1.0000  \n",
       " Mean   :0.5352   Mean   :0.5612   Mean   : 2.498   Mean   :0.5382  \n",
       " 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 3.000   3rd Qu.:1.0000  \n",
       " Max.   :1.0000   Max.   :1.0000   Max.   :67.000   Max.   :1.0000  \n",
       "      A14               A15               R1        \n",
       " Min.   :   0.00   Min.   :     0   Min.   :0.0000  \n",
       " 1st Qu.:  70.75   1st Qu.:     0   1st Qu.:0.0000  \n",
       " Median : 160.00   Median :     5   Median :0.0000  \n",
       " Mean   : 180.08   Mean   :  1013   Mean   :0.4526  \n",
       " 3rd Qu.: 271.00   3rd Qu.:   399   3rd Qu.:1.0000  \n",
       " Max.   :2000.00   Max.   :100000   Max.   :1.0000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(data.table)\n",
    "library(magrittr)\n",
    "library(kernlab)\n",
    "library(kknn)\n",
    "library(matrixStats)\n",
    "\n",
    "data <- fread('credit_card_data-headers.txt')\n",
    "features <- names(data)[-length(data)] \n",
    "class <- names(data)[length(data)]\n",
    "\n",
    "data %>% summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM model outline\n",
    "\n",
    "We apply the model with liner kernel here, using the function `ksvm` under the package `kernlab`. In equation form, the SVM decision boundary is given by: $f(x) = a^T x + b$ where $a \\in \\mathbb{10}$ and $b \\in \\mathbb{R}$. The classification prediction will be given by sign of the discriminary function $f$, i.e. the hyperplane $f = 0$ will be the decision boundary. It is unlikely that the data can be separated linearly on a hard margin, so we opt to train a soft margin SVM. The objective for training is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{a,b} \\sum_i w_i \\max\\{0, 1-(a^T x_i +b)y_i\\} + \\frac{1}{2}\\Vert a \\Vert^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The margins of the SVM for each class are given by the regions $a^T x + b = \\pm 1$, respectively. A soft margin SVM, as per the above equation, does not stricly require no violation in the training data. The space between the two hyperplanes is maximized, while keeping _most_ data points within the correct margin implied by their labels. This trade off is calibrated by the hyper parameter $C$ which implicitly controls the magnitudes for the weights $w_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Some context of function arguments used in `ksvm`:\n",
    "- `type=\"C-svc\"` specifies that the SVM is to be used for classification (hence \"svc\" instead of \"svr\"). Moreover, the regularized with a constant $C$ that specifies cost of violation as in the above equation (as opposed to the percentage of support vectors, $\\nu \\in [0,1]$ in other formulations, see [here](https://www.quora.com/What-is-the-difference-between-C-SVM-and-nu-SVM) for discussion).\n",
    "- `kernel` specifies the kernel function function used in the SVC, which decides the shape of the decision boundary $f = 0$.\n",
    "- `C` is the regularization that determines the cost of classification constraints. High values of $C$ assigns more importance on classifying the data within the margin (hence typically results in a narrower margin length). Since the goal of this exercise is to explore the performance of fitted (optimized) SVM in terms of prediction, we set an arbitrarily high value of $C$, 100 to start with.\n",
    "- `scale=TRUE` dictates that non-binary data be __standardized__ to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T06:58:56.644456Z",
     "start_time": "2021-01-18T06:58:54.444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting default kernel parameters  \n",
      "Support Vector Machine object of class \"ksvm\" \n",
      "\n",
      "SV type: C-svc  (classification) \n",
      " parameter : cost C = 100 \n",
      "\n",
      "Linear (vanilla) kernel function. \n",
      "\n",
      "Number of Support Vectors : 189 \n",
      "\n",
      "Objective Function Value : -17887.92 \n",
      "Training error : 0.136086 \n"
     ]
    }
   ],
   "source": [
    "# apply and train model\n",
    "feat_mat = data[, features, with=FALSE] %>% as.matrix\n",
    "cla_mat = data[, class, with=FALSE] %>% as.matrix\n",
    "\n",
    "model <- ksvm(feat_mat, cla_mat, type=\"C-svc\", kernel=\"vanilladot\", C=100, scale=TRUE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnostics\n",
    "\n",
    "We see from the above model output that the overall error rate is about 13.61%, or 86.39% accuracy. To assess model performance for each label, we computed the confusion matrix below, in terms of counts and percentages. It appears that the SVM is better at identifying 1-labelled data points - the success rates for 0 and 1 are 79.89% and 94.25%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T07:08:17.224265Z",
     "start_time": "2021-01-18T07:08:17.192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R1   0   1\n",
      "1:  0 286  72\n",
      "2:  1  17 279\n"
     ]
    }
   ],
   "source": [
    "# check in-sample fit\n",
    "data[, 'R1_pred' := predict(model, feat_mat)]\n",
    "\n",
    "# in-sample confusion matrix\n",
    "conf_mat <- data[, .N, by=.(R1, R1_pred)] %>% dcast(., R1~R1_pred, value.var=\"N\")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T07:08:17.688884Z",
     "start_time": "2021-01-18T07:08:17.669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R1          0         1\n",
      "1:  0 0.79888268 0.2011173\n",
      "2:  1 0.05743243 0.9425676\n"
     ]
    }
   ],
   "source": [
    "# normalize in terms of %\n",
    "conf_mat[, 2:3] = conf_mat[, 2:3] / conf_mat[, rowSums(.SD), .SDcols=c(2,3)]\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance of variables\n",
    "\n",
    "Note that `model@coef` gives the weights for each data point $\\alpha_i$ in the fitted coefficients $a = \\sum_i \\alpha_i x_i$, where $x_i$ are scaled data points stored in `model@xmatrix`. Check [here](https://www.rdocumentation.org/packages/kernlab/versions/0.9-29/topics/ksvm-class) for a complete list of attributes and methods for the `ksvm` class. So to find the final cofficients for each feature we just need to compute the sum of `model@coef` $\\times$ `model@xmatrix` across all samples (column sum). It appears that `A9` and `A15` are the most determining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T07:18:11.756738Z",
     "start_time": "2021-01-18T07:18:11.734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>A1</dt><dd>-0.00100653481057611</dd><dt>A2</dt><dd>-0.00117290480611665</dd><dt>A3</dt><dd>-0.00162619672236963</dd><dt>A8</dt><dd>0.0030064202649194</dd><dt>A9</dt><dd>1.00494056410556</dd><dt>A10</dt><dd>-0.00282594323043472</dd><dt>A11</dt><dd>0.000260029507016313</dd><dt>A12</dt><dd>-0.000534955143494997</dd><dt>A14</dt><dd>-0.00122837582291523</dd><dt>A15</dt><dd>0.106363399527188</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[A1] -0.00100653481057611\n",
       "\\item[A2] -0.00117290480611665\n",
       "\\item[A3] -0.00162619672236963\n",
       "\\item[A8] 0.0030064202649194\n",
       "\\item[A9] 1.00494056410556\n",
       "\\item[A10] -0.00282594323043472\n",
       "\\item[A11] 0.000260029507016313\n",
       "\\item[A12] -0.000534955143494997\n",
       "\\item[A14] -0.00122837582291523\n",
       "\\item[A15] 0.106363399527188\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "A1\n",
       ":   -0.00100653481057611A2\n",
       ":   -0.00117290480611665A3\n",
       ":   -0.00162619672236963A8\n",
       ":   0.0030064202649194A9\n",
       ":   1.00494056410556A10\n",
       ":   -0.00282594323043472A11\n",
       ":   0.000260029507016313A12\n",
       ":   -0.000534955143494997A14\n",
       ":   -0.00122837582291523A15\n",
       ":   0.106363399527188\n",
       "\n"
      ],
      "text/plain": [
       "           A1            A2            A3            A8            A9 \n",
       "-0.0010065348 -0.0011729048 -0.0016261967  0.0030064203  1.0049405641 \n",
       "          A10           A11           A12           A14           A15 \n",
       "-0.0028259432  0.0002600295 -0.0005349551 -0.0012283758  0.1063633995 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(model@coef[[1]]  *  model@xmatrix[[1]]) %>% colSums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with regularization constant `C`\n",
    "\n",
    "Perhaps we haven't stressed the importance of no violation. We'll increase the cost of violation with higher values of `C` and experiment with a much larger value of 10000. As seen below the accurary did not increase. We know that at some point the training accuracy will stop improving even if $C$ is further increased because beyond that point the optimizer is already ignoring the importance of maximizing the margin, effectively. It seems that for linear kernel a 13.76% error is as good as we can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T05:24:39.234521Z",
     "start_time": "2021-01-18T05:24:19.242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting default kernel parameters  \n",
      "Support Vector Machine object of class \"ksvm\" \n",
      "\n",
      "SV type: C-svc  (classification) \n",
      " parameter : cost C = 10000 \n",
      "\n",
      "Linear (vanilla) kernel function. \n",
      "\n",
      "Number of Support Vectors : 284 \n",
      "\n",
      "Objective Function Value : -1721868 \n",
      "Training error : 0.137615 \n"
     ]
    }
   ],
   "source": [
    "model <- ksvm(feat_mat, cla_mat, type=\"C-svc\", kernel=\"vanilladot\", C=10000, scale=TRUE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer for 2.2 (2)\n",
    "\n",
    "#### Implementing different kernels\n",
    "\n",
    "We proceed to try out all other kernels made available under `ksvm`, including:\n",
    "\n",
    "- `rbfdot`: Radial Basis kernel \"Gaussian\"\n",
    "- `polydot` Polynomial kernel\n",
    "- `vanilladot` Linear kernel\n",
    "- `tanhdot` Hyperbolic tangent kernel\n",
    "- `laplacedot` Laplacian kernel\n",
    "- `besseldot` Bessel kernel\n",
    "- `anovadot` ANOVA RBF kernel\n",
    "- `splinedot` Spline kernel\n",
    "- `stringdot` String kernel\n",
    "\n",
    "In each of the kernel there's a respecting kernel-specific `kpar` to be passed. I experimented by taking values from arbitrary intervals. For simplicity with kernels requiring multiple `kpar` to tune, including Bessel, ANOVA, and string we'll just use the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T06:31:16.519358Z",
     "start_time": "2021-01-18T06:31:06.807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n"
     ]
    }
   ],
   "source": [
    "model_error = list()\n",
    "\n",
    "# rbfdot\n",
    "sigma <- c(.01, .1, .5, 1, 2, 5, 10, 25, 50, 100)\n",
    "rbf_error = list()\n",
    "for (x in sigma) {\n",
    "    model <- ksvm(feat_mat, cla_mat, type=\"C-svc\", kernel=\"rbfdot\", kpar=list(sigma=x), C=100, scale=TRUE)\n",
    "    rbf_error[[x %>% toString]] <- error(model)\n",
    "}\n",
    "model_error[['rbf']] <- rbf_error %>% unlist %>% min\n",
    "rbf_sigma <- which.min(rbf_error) %>% names()\n",
    "\n",
    "# polydot\n",
    "degree <- c(2,3,4,5,6,7)\n",
    "poly_error = list()\n",
    "for (x in degree) {\n",
    "    model <- ksvm(feat_mat, cla_mat, type=\"C-svc\", kernel=\"polydot\", kpar=list(degree=x), C=100, scale=TRUE)\n",
    "    poly_error[[x %>% toString]] <- error(model)\n",
    "}\n",
    "model_error[['poly']] <- poly_error %>% unlist %>% min\n",
    "poly_deg <- which.min(poly_error) %>% names()\n",
    "\n",
    "# tanhdot\n",
    "scale <- c(.01, .1, .5, 1, 2, 5, 10, 25, 50, 100)\n",
    "tanh_error = list()\n",
    "for (x in degree) {\n",
    "    model <- ksvm(feat_mat, cla_mat, type=\"C-svc\", kernel=\"tanhdot\", kpar=list(scale=x), C=100, scale=TRUE)\n",
    "    tanh_error[[x %>% toString]] <- error(model)\n",
    "}\n",
    "model_error[['tanh']] <- tanh_error %>% unlist %>% min\n",
    "tanh_scale <- which.min(tanh_error) %>% names()\n",
    "\n",
    "# laplacedot\n",
    "model <- ksvm(feat_mat, cla_mat, type=\"C-svc\", kernel=\"laplacedot\", C=100, scale=TRUE)\n",
    "model_error[['laplace']] <- error(model)\n",
    "\n",
    "# besseldot\n",
    "model <- ksvm(feat_mat, cla_mat, type=\"C-svc\", kernel=\"besseldot\", C=100, scale=TRUE)\n",
    "model_error[['bessel']] <- error(model)\n",
    "\n",
    "# anovadot\n",
    "model <- ksvm(feat_mat, cla_mat, type=\"C-svc\", kernel=\"anovadot\", C=100, scale=TRUE)\n",
    "model_error[['anova']] <- error(model)\n",
    "\n",
    "# splinedot\n",
    "model <- ksvm(feat_mat, cla_mat, type=\"C-svc\", kernel=\"anovadot\", C=100, scale=TRUE)\n",
    "model_error[['spline']] <- error(model)\n",
    "\n",
    "model_error <- model_error %>% setDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Accuracy Comparison\n",
    "\n",
    "We compare the best error rates for each kernel. We see here that non-linear kernels tend to perform better than vanilladot, with perfect fits in RBF, laplace, and polynomial. However it is acknowledged that perfect fit does not equate best model - might just be overfitting the data. This can be seen observing that the polynomial kernel started attaining perfect fit when degree is larger than or equal to 5 - when the shape of decision boundary can take more arbitrary shapes that perfectly separate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T06:28:59.468330Z",
     "start_time": "2021-01-18T06:28:59.449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rbf poly     tanh laplace     bessel      anova     spline\n",
      "1:   0    0 0.266055       0 0.07492355 0.09327217 0.09327217\n"
     ]
    }
   ],
   "source": [
    "print(model_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer for 2.2 (3)\n",
    "\n",
    "#### Outline of KNN\n",
    "\n",
    "Instead of outlining a KNN classifier with mathematical syntax, we list the algorithm steps as follows:\n",
    "1. Choose a natural number $k$ and a distance metric (Manhattan, Eucliean, generalized Minkowski etc). \tTo prevent bias against large scales, features must be standardized or scaled.\n",
    "2. For each point to be classified, find the k nearest samples with respect to the chosen metric.\n",
    "3. Prediction of class labels by majority vote.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "We train a KNN classifier with `kknn` function defined under the `kknn` package. Some context of function arguments:\n",
    "- `distance`: Minkowski distance parameter, determines how distance is defined.\n",
    "- `k`: size of neighborhood $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
