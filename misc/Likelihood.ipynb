{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few notes on Likelihood Functions\n",
    "\n",
    "## Preliminary\n",
    "\n",
    "This notebook is intended to serve as a memo to outline some definitions and properties of the likelihood function (and MLE estimation thereof) that will be useful in Econometrics analyses, or machine learning applications. It would perhaps be useful to define the framework that will be used:\n",
    "\n",
    "- Let $X$ be a random variable. For simplicity, we consider random variables with support $\\Omega(X) \\subseteq \\mathbb{R}^k$. \n",
    "- Suppose $X \\sim F(x; \\theta)$, i.e. it follows a distribution $F$ parameterized by a set of real-valued parameter $\\theta \\in \\Theta$, the parameter space. By convention, $F$ is the probability mass function if $X$ is discrete. Where $X$ is continuous, $F$ denotes the cumulative distribution function with well-defined density $f$. \n",
    "- We can perhaps extend this setup, hopefully without the loss of generality to a broader interpretation. Where $\\theta$ will represent a __hypothesis__ about __the model__ $F$ about the data generating process $X$, which in this case is a distribution.\n",
    "- Denote $(x_1, x_2, \\ldots, x_n)$ the data/observations arising from an independent, identically distributed sampling mechanism from $X$. This will be the set of \"events\" that we are interested in explaining and model on.\n",
    "\n",
    "## Probability and Likelihood\n",
    "\n",
    "Porbability and likelihood are like flip sides of the same coin. A probability measure is a real-valued function defined on the event space of $X$. In order to define the probability of observing $(x_1, x_2, \\ldots, x_n)$ properly, the exact distribution $F(x; \\theta)$, hence $\\theta$ will have to be known. i.e. probabilities are functions of realization of $x$ (events), conditional on $F$ and $\\theta$ (distribution and hypothesis). \n",
    "\n",
    "On the other hand, a likelihood function is a real-valued function defined on the hypothesis space $\\Theta$ and it serves to measure the goodness-of-fit of $F(\\theta|x_1, x_2, \\dots, x_n)$ on the observations $(x_1, x_2, \\dots, x_n)$ for a candidate hypothesis $\\theta$. i.e. likelihoods are functions of $\\theta$ (hypotheses/distribution parameters) conditional on $F$ and realizations $x$. The way how \"goodness-of-fit\" for a candidate hypothesis $\\theta_0$ is accessed is by evaluating the probability of observing the data if the data is generated $F(\\theta_0)$. Naturally, we would like to find a parameter $\\theta^*$ such that the probability of observing $x$ is maximized, i.e. $\\theta^* = \\text{argmax}_\\theta L(\\theta|x)$. Such $\\theta^*$ will be the MLE estimate. In econometrics terminology, we say that $\\theta^*$ is (point) __identified__ if it is unique.\n",
    "\n",
    "## Discrete Example\n",
    "\n",
    "A coin following binomial($p$) results in $n$ outcomes $(x_1, x_2, \\dots, x_n)$, resulting in $y_0$ and $y_1$ zeros and ones. If we are confident that the sample is i.i.d. (which it should be), the probability of observing the data is $P(x|p) = \\prod_{i:x_i=1}p\\prod_{i:x_i=0}(1-p) = p^{y_1}(1-p)^{y_0}$ given $p$. The likelihood of the hypothesis that the coin-flipping process follows binomial($r$) is $L(r|x) = \\prod_{i:x_i=1}r\\prod_{i:x_i=0}(1-r) = r^{y_1}(1-r)^{y_0}$. \n",
    "\n",
    "## Continuous Example\n",
    "\n",
    "A 1-D data vector $X$ is known to be sampled from $N(\\mu, \\sigma^2)$. We would like to find the parameter vector $\\theta \\triangleq (\\mu, \\sigma^2)$ that maximizes the likelihood. It is well-known that the likelihood function of $\\theta$ conditional of observations is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\theta; x) &= \\prod_{i=1}^n f(x_i; \\theta) \\\\\n",
    "&= \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\Bigg( -\\frac{1}{2}\\Big(\\frac{x_i-\\mu}{\\sigma}\\Big)^2 \\Bigg)\n",
    "\\end{align*}\n",
    "\n",
    "It is perhaps a bit unclear to some (myself included) how this compares to the discrete case which is intuitive in terms of probability (to begin with, the _probability_ of observing particular outcomes from continuous random variables is zero). The most orthodox way I found to interpret this is to invoke a particular definition of the probability density function $f(x)$: \n",
    "\n",
    "(from Wikipedia) _it is a relative likelihood that the value of the random variable would equal that sample. In other words, the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would equal one sample compared to the other sample_. I\n",
    "\n",
    "Now because the sample is i.i.d., the relative likelihood which is the PDF of the sample will be the product of the PDF for each sample point. I find [this video](https://www.youtube.com/watch?v=XepXtl9YKwc) by StatQuest explains pretty well how $f(x;\\theta)$ is a normalized index to compare the likelihood of $x$ with another candidate hypothesis $f(x;\\phi)$. \n",
    "\n",
    "However, I still find this interpretation a bit circular to me. Here is a __heuristic__, perhaps more intuitive interpretation I propose:\n",
    "\n",
    "__Warning: not validated - just for rough understanding__\n",
    "\n",
    "While the probability of observing a particular value $z$ from a continuous distribution is zero, we can find the probability of observing a realization over a small neighborhood of $z$: $P\\big(x \\in (z-\\delta, z+\\delta)\\big) = F(x+\\delta; \\theta)-F(x-\\delta; \\theta) \\approx 2\\delta f(x; \\theta)$. Because the likelihood function valued at $\\theta$ is the goodness-of-fit (in terms of probability of observing $x$), we simply write the approximate likelihood as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{L}(\\theta, \\delta; x) &= \\prod_i 2\\delta f(x_i; \\theta) \\\\\n",
    "&\\propto \\prod_i f(x_i; \\theta) \\\\\n",
    "&= L(\\theta; x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "Suppose we observe data $\\{X_i, Y_i\\}_{i=1}^N$, and the model is:\n",
    "\n",
    "\\begin{align*}\n",
    "Y_i &= \\beta X_i + \\varepsilon_i\\\\\n",
    "\\varepsilon &\\sim N(0,\\sigma)\n",
    "\\end{align*}\n",
    "\n",
    "Normality of the error term can be by assumption, or simply arising from central limit theorem on big data. The goal of is to find $(\\beta, \\sigma)$ that maximizes the likelihood of the observed data. Note that this is equivalent to finding the likelihood of observing $\\{\\varepsilon_i\\}_{i=1}^N$ with values $(Y_1-\\beta X_1, Y_2-\\beta X_2, \\ldots, Y_N-\\beta X_N)$. Denote the normal pdf as $f$, the likelihood function for $(\\beta, \\sigma)$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\beta, \\sigma|X,Y) = \\prod_{i=1}^N f(Y_i-\\beta X_i; \\beta, \\sigma)\n",
    "\\end{align*}\n",
    "\n",
    "As well-noted in the literature, this is an equivalent problem to OLS.\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Suppose we observe data $\\{X_i, Y_i\\}_{i=1}^N$ where $Y_i$ is binary. We can consider the likelihood in terms of observing a data of i.i.d. __discrete outcomes__, i.e. we express the model as:\n",
    "\n",
    "\\begin{align*}\n",
    "Y_i =   \\begin{cases}\n",
    "    1 & \\text{with prob   } \\Lambda(\\beta X_i) \\\\\n",
    "    0, & \\text{with prob   } 1-\\Lambda(\\beta X_i) \\\\\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Write $\\Lambda(z) = \\frac{\\exp(z)}{1+\\exp(z)}$. The discrete interpretation implies likelihood function as:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\beta|x) = \\prod_{i:Y_i=1} \\Lambda(\\beta X_i) \\prod_{i:Y_i=0} \\big(1-\\Lambda(\\beta X_i)\\big)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Some may have a score-based interpretation, implying a __continuous latent__ variable $Y^*$. The specification is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "Y^*_i &= \\beta X_i + \\varepsilon_i\\\\\n",
    "\\varepsilon_i &\\sim \\text{Gumbel}(1)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The Gubmel(1) is also known as EVT1 or simply the logistic distribution. $Y^*$ is interpretated as the score of alternative 1. The score of alternative 0 is normalized at zero. The outcome is 1 if the score of alternative 1 is greater than that of alternative 0, i.e. $Y^*_i = \\beta X_i + \\varepsilon_i > 0$. The cdf of Gumbel distribution is $\\Lambda$. The likelihood function for is:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\beta|X) &= \\prod_{i:Y_i=1} P(Y^*_i \\geq 0) \\prod_{i:Y_i=0} P(Y^*_i < 0)  \\\\\n",
    "&= \\prod_{i:Y_i=1} P(\\varepsilon_i \\geq -\\beta X_i) \\prod_{i:Y_i=0} P(\\varepsilon_i < -\\beta X_i)\\\\\n",
    "&= \\prod_{i:Y_i=1} \\big(1-\\Lambda(-\\beta X_i)\\big) \\prod_{i:Y_i=0} \\Lambda(-\\beta X_i)\\\\\n",
    "&\\triangleq \\prod_{i:Y_i=1} \\Lambda(\\beta X_i) \\prod_{i:Y_i=0} \\big(1-\\Lambda(\\beta X_i)\\big)\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
